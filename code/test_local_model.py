#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½
éªŒè¯BAAI/bge-small-zh-v1.5æ¨¡å‹æ˜¯å¦èƒ½ä»æœ¬åœ°æ­£ç¡®åŠ è½½
"""

import os
import sys
from FlagEmbedding import FlagModel

def test_local_model():
    """
    æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½
    """
    print("=== æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½ ===")
    
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(current_dir, "model")
    
    print(f"å½“å‰ç›®å½•: {current_dir}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        "config.json",
        "pytorch_model.bin", 
        "tokenizer.json",
        "special_tokens_map.json"
    ]
    
    print("\næ£€æŸ¥æ¨¡å‹æ–‡ä»¶:")
    for file_name in required_files:
        file_path = os.path.join(model_path, file_name)
        exists = os.path.exists(file_path)
        print(f"  {file_name}: {'âœ“' if exists else 'âœ—'}")
        if not exists:
            print(f"    é”™è¯¯: ç¼ºå°‘æ–‡ä»¶ {file_path}")
            return False
    
    # å°è¯•åŠ è½½æ¨¡å‹
    try:
        print("\næ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹...")
        model = FlagModel(
            model_path,
            query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
            use_fp16=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # æµ‹è¯•ç¼–ç åŠŸèƒ½
        print("\næµ‹è¯•ç¼–ç åŠŸèƒ½...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
        embedding = model.encode([test_text])
        print(f"âœ“ ç¼–ç æˆåŠŸ! å‘é‡ç»´åº¦: {embedding.shape}")
        
        # æµ‹è¯•æŸ¥è¯¢ç¼–ç 
        print("\næµ‹è¯•æŸ¥è¯¢ç¼–ç åŠŸèƒ½...")
        query_embedding = model.encode_queries(["æµ‹è¯•æŸ¥è¯¢"])
        print(f"âœ“ æŸ¥è¯¢ç¼–ç æˆåŠŸ! å‘é‡ç»´åº¦: {query_embedding.shape}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´ä¸‹è½½")
        print("2. æ£€æŸ¥FlagEmbeddingåº“æ˜¯å¦æ­£ç¡®å®‰è£…: pip install FlagEmbedding")
        print("3. æ£€æŸ¥PyTorchæ˜¯å¦æ­£ç¡®å®‰è£…")
        return False

def test_vectorizer_with_local_model():
    """
    æµ‹è¯•å‘é‡åŒ–å™¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    """
    print("\n=== æµ‹è¯•å‘é‡åŒ–å™¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ ===")
    
    try:
        from vectorize_chunks import ChunkVectorizer
        
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
        vectorizer = ChunkVectorizer(model_name="e:\\PyProjects\\QASystem\\code\\model")
        vectorizer.load_model()
        
        print("âœ“ å‘é‡åŒ–å™¨åŠ è½½æœ¬åœ°æ¨¡å‹æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âœ— å‘é‡åŒ–å™¨åŠ è½½æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•æœ¬åœ°æ¨¡å‹...\n")
    
    # æµ‹è¯•ç›´æ¥åŠ è½½
    success1 = test_local_model()
    
    # æµ‹è¯•å‘é‡åŒ–å™¨
    success2 = test_vectorizer_with_local_model()
    
    print("\n=== æµ‹è¯•ç»“æœ ===")
    print(f"ç›´æ¥åŠ è½½æ¨¡å‹: {'âœ“ æˆåŠŸ' if success1 else 'âœ— å¤±è´¥'}")
    print(f"å‘é‡åŒ–å™¨åŠ è½½: {'âœ“ æˆåŠŸ' if success2 else 'âœ— å¤±è´¥'}")
    
    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! æœ¬åœ°æ¨¡å‹é…ç½®æˆåŠŸ!")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")