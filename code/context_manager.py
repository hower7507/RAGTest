# -*- coding: utf-8 -*-
"""
ä¸Šä¸‹æ–‡ç®¡ç†å™¨
å¤„ç†æœç´¢ç»“æœçš„å»é‡ã€é‡æ’åºå’Œä¸Šä¸‹æ–‡æ‹¼æ¥
"""

from typing import Dict, List, Any, Optional
from collections import defaultdict
import os
from advanced_search_system import AdvancedSearchSystem
from vectorize_chunks import ChunkVectorizer

class ContextManager:
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šå¤„ç†æœç´¢ç»“æœçš„æ•´åˆå’Œä¼˜åŒ–
    """
    
    def __init__(self, 
                 search_system: AdvancedSearchSystem,
                 collection: Any, # chromadb.Collection
                 use_bre_reranking: bool = True):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            vectorizer: å‘é‡åŒ–å™¨å®ä¾‹
            use_bre_reranking: æ˜¯å¦ä½¿ç”¨BREé‡æ’åº
        """
        self.search_system = search_system
        self.collection = collection
        self.use_bre_reranking = use_bre_reranking
        
        # BREé‡æ’åºç³»ç»Ÿç°åœ¨ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„search_system
        self.bre_system = self.search_system if use_bre_reranking else None
        
        print(f"ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ŒBREé‡æ’åº: {use_bre_reranking}")
    
    def deduplicate_by_chunk_id(self, 
                               search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åŸºäºchunk_idè¿›è¡Œå»é‡
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            å»é‡åçš„ç»“æœåˆ—è¡¨
        """
        print(f"å¼€å§‹å»é‡ï¼ŒåŸå§‹ç»“æœæ•°: {len(search_results)}")
        
        # æŒ‰chunk_idåˆ†ç»„ï¼Œä¿ç•™ä¼˜å…ˆçº§æœ€é«˜çš„ç»“æœ
        chunk_groups = defaultdict(list)
        
        for result in search_results:
            # è·å–chunk_id
            chunk_id = self._extract_chunk_id(result)
            if chunk_id is not None:
                chunk_groups[chunk_id].append(result)
            else:
                # å¦‚æœæ— æ³•æå–chunk_idï¼Œä½¿ç”¨resultçš„id
                result_id = result.get('id', f"unknown_{len(chunk_groups)}")
                chunk_groups[result_id].append(result)
        
        # ä¸ºæ¯ä¸ªchunk_idé€‰æ‹©æœ€ä½³ç»“æœ
        deduplicated_results = []
        
        for chunk_id, results in chunk_groups.items():
            if len(results) == 1:
                deduplicated_results.append(results[0])
            else:
                # å¤šä¸ªç»“æœæ—¶ï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„
                best_result = self._select_best_result(results)
                deduplicated_results.append(best_result)
        
        print(f"å»é‡å®Œæˆï¼Œç»“æœæ•°: {len(deduplicated_results)}")
        return deduplicated_results
    
    def _extract_chunk_id(self, result: Dict[str, Any]) -> Optional[int]:
        """
        ä»æœç´¢ç»“æœä¸­æå–chunk_id
        
        Args:
            result: æœç´¢ç»“æœ
            
        Returns:
            chunk_idæˆ–None
        """
        # å°è¯•ä»metadataä¸­è·å–
        metadata = result.get('metadata', {})
        if 'chunk_id' in metadata:
            return metadata['chunk_id']
        
        # å°è¯•ä»idä¸­è§£æ
        result_id = result.get('id', '')
        if isinstance(result_id, str):
            # å¤„ç† chapXX-N æ ¼å¼
            if '-' in result_id:
                try:
                    chunk_part = result_id.split('-')[-1]  # å–æœ€åä¸€éƒ¨åˆ†
                    return int(chunk_part)
                except (ValueError, IndexError):
                    pass
            # å¤„ç† chunk_N æ ¼å¼
            elif result_id.startswith('chunk_'):
                try:
                    return int(result_id.split('_')[1])
                except (IndexError, ValueError):
                    pass
        
        # å°è¯•ä»document_idä¸­è·å–
        doc_id = result.get('document_id', '')
        if isinstance(doc_id, str):
            # å¤„ç† chapXX-N æ ¼å¼
            if '-' in doc_id:
                try:
                    chunk_part = doc_id.split('-')[-1]  # å–æœ€åä¸€éƒ¨åˆ†
                    return int(chunk_part)
                except (ValueError, IndexError):
                    pass
            # å¤„ç† chunk_N æ ¼å¼
            elif doc_id.startswith('chunk_'):
                try:
                    return int(doc_id.split('_')[1])
                except (IndexError, ValueError):
                    pass
        
        return None
    
    def _select_best_result(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ä»å¤šä¸ªç›¸åŒchunk_idçš„ç»“æœä¸­é€‰æ‹©æœ€ä½³çš„
        
        Args:
            results: ç›¸åŒchunk_idçš„ç»“æœåˆ—è¡¨
            
        Returns:
            æœ€ä½³ç»“æœ
        """
        # æŒ‰æœç´¢ä¼˜å…ˆçº§æ’åºï¼ˆå‘é‡æœç´¢ > å…³é”®è¯æœç´¢ > æ—¶é—´æœç´¢ï¼‰
        priority_order = {
            'vector_search': 1,
            'keyword_search': 2,
            'time_search': 3
        }
        
        # æŒ‰ä¼˜å…ˆçº§å’Œå¾—åˆ†æ’åº
        sorted_results = sorted(
            results,
            key=lambda x: (
                priority_order.get(x.get('search_source', 'unknown'), 999),
                -x.get('score', 0)  # å¾—åˆ†é™åº
            )
        )
        
        best_result = sorted_results[0]
        
        # èåˆå¾—åˆ†ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ¥æºï¼‰
        if len(results) > 1:
            total_score = sum(r.get('score', 0) for r in results)
            best_result['fused_score'] = total_score
            best_result['source_count'] = len(results)
            best_result['all_sources'] = [r.get('search_source', 'unknown') for r in results]
        
        return best_result
    
    def apply_bre_reranking(self,
                          query: str,
                          search_results: List[Dict[str, Any]],
                          top_k: int = 10) -> List[Dict[str, Any]]:
        """
        åº”ç”¨BREé‡æ’åºï¼ˆå·²ç¦ç”¨ - ä»…å»é‡å’Œæˆªå–ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            search_results: æœç´¢ç»“æœ
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            å»é‡å¹¶æˆªå–åçš„ç»“æœï¼ˆè·³è¿‡BREé‡æ’åºï¼‰
        """
        print("ğŸš« BREé‡æ’åºå·²è¢«å¼ºåˆ¶ç¦ç”¨ï¼Œä»…è¿›è¡Œå»é‡å’Œæˆªå–æ“ä½œ")
        
        if not search_results:
            print("âš ï¸ æœç´¢ç»“æœä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        print(f"ğŸ“¥ è¾“å…¥ç»“æœæ•°: {len(search_results)}")
        
        # å…ˆè¿›è¡Œå»é‡ï¼ˆåŸºäºchunk_idæˆ–document_idï¼‰
        deduplicated_results = self.deduplicate_by_chunk_id(search_results)
        print(f"ğŸ”„ å»é‡åç»“æœæ•°: {len(deduplicated_results)}")
        
        # æˆªå–åˆ°æŒ‡å®šæ•°é‡
        final_results = deduplicated_results[:top_k]
        print(f"âœ‚ï¸ æˆªå–åæœ€ç»ˆç»“æœæ•°: {len(final_results)}")
        
        # ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰å¿…è¦çš„å­—æ®µ
        for i, result in enumerate(final_results):
            if 'final_rank' not in result:
                result['final_rank'] = i + 1
            if 'final_score' not in result:
                result['final_score'] = result.get('score', 0.0)
        
        print(f"âœ… å¤„ç†å®Œæˆï¼šä» {len(search_results)} ä¸ªç»“æœ â†’ å»é‡åˆ° {len(deduplicated_results)} ä¸ª â†’ æœ€ç»ˆè¿”å› {len(final_results)} ä¸ªç»“æœ")
        
        return final_results
    
    def build_context(self, 
                     search_results: List[Dict[str, Any]],
                     max_context_length: int = 2000) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        
        Args:
            search_results: æœç´¢ç»“æœ
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            
        Returns:
            æ„å»ºçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not search_results:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
        
        context_parts = []
        current_length = 0
        
        for i, result in enumerate(search_results):
            content = result.get('content', '')
            metadata = result.get('metadata', {})
            
            # æ ¹æ®æ•°æ®æ ¼å¼åŠ¨æ€æ ¼å¼åŒ–ç»“æœ
            if 'question' in metadata and 'answer' in metadata:
                # QAæ ¼å¼æ•°æ®
                source_file = metadata.get('source_file', '')
                # æå–æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
                if source_file:
                    file_name = os.path.basename(source_file).replace('.txt', '').replace('_processed.json', '')
                else:
                    file_name = 'æœªçŸ¥æ¥æº'
                chunk_id = self._extract_chunk_id(result)
                # å¦‚æœchunk_idä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                chunk_id_str = str(chunk_id) if chunk_id is not None else ""
                result_text = f"""[å‚è€ƒç‰‡æ®µ{file_name}-{chunk_id_str} - æ¥æº: {file_name}]
é—®é¢˜: {metadata.get('question', '')}
ç­”æ¡ˆ: {metadata.get('answer', '')}
ç›¸å…³åº¦: {result.get('bre_score', result.get('score', 0)):.3f}
"""
            else:
                # ä¼ ç»Ÿæ—¶é—´æˆ³æ ¼å¼æ•°æ®
                source_file = metadata.get('source_file', '')
                # æå–æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
                if source_file:
                    file_name = os.path.basename(source_file).replace('.txt', '').replace('_processed.json', '')
                else:
                    file_name = 'æœªçŸ¥æ¥æº'
                
                chunk_id = self._extract_chunk_id(result)
                # å¦‚æœchunk_idä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                chunk_id_str = str(chunk_id) if chunk_id is not None else ""
                result_text = f"""[{file_name}-{chunk_id_str} - æ¥æº: {file_name}]
æ—¶é—´: {metadata.get('start_time', '')} - {metadata.get('end_time', '')}
è¯´è¯äºº: {metadata.get('speakers', 'æœªçŸ¥')}
å†…å®¹: {content}
ç›¸å…³åº¦: {result.get('bre_score', result.get('score', 0)):.3f}
"""
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(result_text) > max_context_length:
                break
            
            context_parts.append(result_text)
            current_length += len(result_text)
        
        return "\n".join(context_parts)
    
    def process_search_results(self, 
                             query: str,
                             search_results: List[Dict[str, Any]],
                             max_results: int = 10,
                             max_context_length: int = 2000) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æœç´¢ç»“æœå¤„ç†æµç¨‹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            search_results: åŸå§‹æœç´¢ç»“æœ
            max_results: æœ€å¤§ç»“æœæ•°é‡
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            
        Returns:
            å¤„ç†åçš„ç»“æœå’Œä¸Šä¸‹æ–‡
        """
        print(f"\n=== å¼€å§‹å¤„ç†æœç´¢ç»“æœ ===")
        print(f"åŸå§‹ç»“æœæ•°: {len(search_results)}")
        
        # 1. å»é‡
        deduplicated_results = self.deduplicate_by_chunk_id(search_results)
        
        # 2. BREé‡æ’åº
        reranked_results = self.apply_bre_reranking(
            query=query,
            search_results=deduplicated_results,
            top_k=max_results
        )
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = self.build_context(
            search_results=reranked_results,
            max_context_length=max_context_length
        )
        
        print(f"=== æœç´¢ç»“æœå¤„ç†å®Œæˆ ===")
        print(f"æœ€ç»ˆç»“æœæ•°: {len(reranked_results)}")
        print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)}å­—ç¬¦")
        
        return {
            'processed_results': reranked_results,
            'context': context,
            'stats': {
                'original_count': len(search_results),
                'deduplicated_count': len(deduplicated_results),
                'final_count': len(reranked_results),
                'context_length': len(context)
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨
    context_manager = ContextManager(use_bre_reranking=False)  # æš‚æ—¶ä¸ä½¿ç”¨BRE
    
    # æ¨¡æ‹Ÿæœç´¢ç»“æœ
    mock_results = [
        {
            'id': 'chunk_1',
            'content': 'è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹',
            'metadata': {'chunk_id': 1, 'start_time': '00:01', 'speakers': 'è¯´è¯äºº1'},
            'score': 0.8,
            'search_source': 'vector_search'
        },
        {
            'id': 'chunk_1',  # é‡å¤çš„chunk_id
            'content': 'è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹ï¼ˆé‡å¤ï¼‰',
            'metadata': {'chunk_id': 1, 'start_time': '00:01', 'speakers': 'è¯´è¯äºº1'},
            'score': 0.6,
            'search_source': 'keyword_search'
        },
        {
            'id': 'chunk_2',
            'content': 'è¿™æ˜¯ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹',
            'metadata': {'chunk_id': 2, 'start_time': '00:05', 'speakers': 'è¯´è¯äºº2'},
            'score': 0.7,
            'search_source': 'vector_search'
        }
    ]
    
    # å¤„ç†æœç´¢ç»“æœ
    result = context_manager.process_search_results(
        query="æµ‹è¯•æŸ¥è¯¢",
        search_results=mock_results,
        max_results=5
    )
    
    print("\nå¤„ç†ç»“æœ:")
    print(f"ç»Ÿè®¡ä¿¡æ¯: {result['stats']}")
    print(f"\nä¸Šä¸‹æ–‡:\n{result['context']}")