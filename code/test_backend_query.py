# -*- coding: utf-8 -*-
"""
æµ‹è¯•åå°æŸ¥è¯¢åŠŸèƒ½
æ¨¡æ‹Ÿå®é™…æŸ¥è¯¢è¿‡ç¨‹ï¼Œè¯Šæ–­æŸ¥è¯¢é—®é¢˜
"""

import sys
import os
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from search_interface import SearchInterface
from vectorize_chunks import ChunkVectorizer

def test_direct_chromadb_query():
    """
    ç›´æ¥æµ‹è¯•ChromaDBæŸ¥è¯¢
    """
    print("=" * 80)
    print("ğŸ” ç›´æ¥æµ‹è¯•ChromaDBæŸ¥è¯¢")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–å‘é‡åŒ–å™¨
        vectorizer = ChunkVectorizer(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            collection_name="qa_system_chunks"
        )
        
        # è¿æ¥åˆ°ä¸»æ•°æ®åº“
        main_db_path = "e:\\PyProjects\\QASystem\\chroma_db"
        vectorizer.init_chromadb(main_db_path)
        
        print(f"âœ… æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {main_db_path}")
        
        # è·å–é›†åˆä¿¡æ¯
        info = vectorizer.get_collection_info()
        print(f"ğŸ“Š é›†åˆä¿¡æ¯: {info}")
        
        # æµ‹è¯•ç®€å•æŸ¥è¯¢
        print("\nğŸ” æµ‹è¯•ç®€å•æŸ¥è¯¢...")
        test_queries = [
            "è‡ªç„¶è¯­è¨€å¤„ç†",
            "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ ",
            "äººå·¥æ™ºèƒ½",
            "æ•°æ®æŒ–æ˜"
        ]
        
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            try:
                # ç›´æ¥ä½¿ç”¨ChromaDBæŸ¥è¯¢
                results = vectorizer.collection.query(
                    query_texts=[query],
                    n_results=5,
                    include=["documents", "metadatas", "distances"]
                )
                
                if results and 'documents' in results and results['documents'][0]:
                    print(f"  âœ… æ‰¾åˆ° {len(results['documents'][0])} ä¸ªç»“æœ")
                    for i, (doc, metadata, distance) in enumerate(zip(
                        results['documents'][0][:3],  # åªæ˜¾ç¤ºå‰3ä¸ª
                        results['metadatas'][0][:3],
                        results['distances'][0][:3]
                    )):
                        print(f"    ç»“æœ {i+1}:")
                        print(f"      è·ç¦»: {distance:.4f}")
                        print(f"      å†…å®¹: {doc[:50]}...")
                        print(f"      æ¥æº: {metadata.get('source', 'unknown')}")
                        print(f"      ç±»å‹: {metadata.get('chunk_type', 'unknown')}")
                else:
                    print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
                    
            except Exception as e:
                print(f"  âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›´æ¥ChromaDBæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_search_interface():
    """
    æµ‹è¯•æœç´¢æ¥å£
    """
    print("\n" + "=" * 80)
    print("ğŸ” æµ‹è¯•æœç´¢æ¥å£")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–æœç´¢æ¥å£
        search_interface = SearchInterface(config_name="balanced")
        
        # åˆå§‹åŒ–
        if not search_interface.initialize():
            print("âŒ æœç´¢æ¥å£åˆå§‹åŒ–å¤±è´¥")
            return False
        
        print("âœ… æœç´¢æ¥å£åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ä»€ä¹ˆ",
            "æœºå™¨å­¦ä¹ ç®—æ³•",
            "æ·±åº¦å­¦ä¹ æ¨¡å‹",
            "äººå·¥æ™ºèƒ½åº”ç”¨"
        ]
        
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            try:
                result = search_interface.search(
                    query=query,
                    top_k=5,
                    return_prompt=False
                )
                
                if 'error' in result:
                    print(f"  âŒ æŸ¥è¯¢é”™è¯¯: {result['error']}")
                elif result.get('total_results', 0) > 0:
                    print(f"  âœ… æ‰¾åˆ° {result['total_results']} ä¸ªç»“æœ")
                    print(f"  ğŸ• æœç´¢æ—¶é—´: {result.get('search_time', 0):.3f}ç§’")
                    print(f"  ğŸ“Š å€™é€‰æ–‡æ¡£: {result.get('total_candidates', 0)}")
                    
                    # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                    for i, res in enumerate(result['results'][:3]):
                        print(f"    ç»“æœ {i+1}:")
                        print(f"      å¾—åˆ†: {res.get('score', 0):.4f}")
                        print(f"      å†…å®¹: {res['content'][:50]}...")
                        metadata = res.get('metadata', {})
                        print(f"      æ¥æº: {metadata.get('source', metadata.get('source_file', 'unknown'))}")
                        print(f"      ç±»å‹: {metadata.get('chunk_type', 'unknown')}")
                else:
                    print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
                    print(f"  è¯¦ç»†ä¿¡æ¯: {result}")
                    
            except Exception as e:
                print(f"  âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        return True
        
    except Exception as e:
        print(f"âŒ æœç´¢æ¥å£æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_specific_metadata_query():
    """
    æµ‹è¯•ç‰¹å®šå…ƒæ•°æ®æŸ¥è¯¢
    """
    print("\n" + "=" * 80)
    print("ğŸ” æµ‹è¯•ç‰¹å®šå…ƒæ•°æ®æŸ¥è¯¢")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–å‘é‡åŒ–å™¨
        vectorizer = ChunkVectorizer(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            collection_name="qa_system_chunks"
        )
        
        # è¿æ¥åˆ°ä¸»æ•°æ®åº“
        main_db_path = "e:\\PyProjects\\QASystem\\chroma_db"
        vectorizer.init_chromadb(main_db_path)
        
        # æµ‹è¯•æŒ‰æ¥æºè¿‡æ»¤
        print("\nğŸ” æµ‹è¯•æŒ‰æ¥æºè¿‡æ»¤...")
        sources_to_test = ['chap01.txt', 'chap02.txt']
        
        for source in sources_to_test:
            print(f"\næŸ¥æ‰¾æ¥æºä¸º '{source}' çš„æ–‡æ¡£:")
            try:
                # ä½¿ç”¨whereæ¡ä»¶æŸ¥è¯¢
                results = vectorizer.collection.get(
                    where={"source": source},
                    limit=10,
                    include=["documents", "metadatas"]
                )
                
                if results and 'documents' in results and results['documents']:
                    print(f"  âœ… æ‰¾åˆ° {len(results['documents'])} ä¸ªç»“æœ")
                    for i, (doc, metadata) in enumerate(zip(
                        results['documents'][:3],
                        results['metadatas'][:3]
                    )):
                        print(f"    æ–‡æ¡£ {i+1}:")
                        print(f"      å†…å®¹: {doc[:50]}...")
                        print(f"      æ¥æº: {metadata.get('source', 'unknown')}")
                        print(f"      ç±»å‹: {metadata.get('chunk_type', 'unknown')}")
                        print(f"      å­—æ•°: {metadata.get('word_count', 'unknown')}")
                else:
                    print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°æ¥æºä¸º '{source}' çš„æ–‡æ¡£")
                    
                # å°è¯•ä½¿ç”¨source_fileå­—æ®µ
                print(f"  å°è¯•ä½¿ç”¨source_fileå­—æ®µæŸ¥æ‰¾ '{source}':")
                results2 = vectorizer.collection.get(
                    where={"source_file": source},
                    limit=10,
                    include=["documents", "metadatas"]
                )
                
                if results2 and 'documents' in results2 and results2['documents']:
                    print(f"    âœ… é€šè¿‡source_fileæ‰¾åˆ° {len(results2['documents'])} ä¸ªç»“æœ")
                else:
                    print(f"    âŒ é€šè¿‡source_fileä¹Ÿæ²¡æœ‰æ‰¾åˆ°ç»“æœ")
                    
            except Exception as e:
                print(f"  âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        
        # æµ‹è¯•æŒ‰ç±»å‹è¿‡æ»¤
        print("\nğŸ” æµ‹è¯•æŒ‰ç±»å‹è¿‡æ»¤...")
        chunk_types = ['general_text', 'qa_pair', 'traditional']
        
        for chunk_type in chunk_types:
            print(f"\næŸ¥æ‰¾ç±»å‹ä¸º '{chunk_type}' çš„æ–‡æ¡£:")
            try:
                results = vectorizer.collection.get(
                    where={"chunk_type": chunk_type},
                    limit=5,
                    include=["documents", "metadatas"]
                )
                
                if results and 'documents' in results and results['documents']:
                    print(f"  âœ… æ‰¾åˆ° {len(results['documents'])} ä¸ªç»“æœ")
                else:
                    print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°ç±»å‹ä¸º '{chunk_type}' çš„æ–‡æ¡£")
                    
            except Exception as e:
                print(f"  âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å…ƒæ•°æ®æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    """
    print(f"ğŸš€ å¼€å§‹åå°æŸ¥è¯¢æµ‹è¯•")
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # æµ‹è¯•1: ç›´æ¥ChromaDBæŸ¥è¯¢
    success1 = test_direct_chromadb_query()
    
    # æµ‹è¯•2: æœç´¢æ¥å£
    success2 = test_search_interface()
    
    # æµ‹è¯•3: ç‰¹å®šå…ƒæ•°æ®æŸ¥è¯¢
    success3 = test_specific_metadata_query()
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print(f"ç›´æ¥ChromaDBæŸ¥è¯¢: {'âœ… æˆåŠŸ' if success1 else 'âŒ å¤±è´¥'}")
    print(f"æœç´¢æ¥å£æµ‹è¯•: {'âœ… æˆåŠŸ' if success2 else 'âŒ å¤±è´¥'}")
    print(f"å…ƒæ•°æ®æŸ¥è¯¢æµ‹è¯•: {'âœ… æˆåŠŸ' if success3 else 'âŒ å¤±è´¥'}")
    
    if all([success1, success2, success3]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()